{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install openai\n",
    "!python3 -m pip install gradio\n",
    "!python3 -m pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "model_base_url = \"http://llama3/v1\"\n",
    "\n",
    "client_agent = OpenAI(\n",
    "        base_url=model_base_url,\n",
    "        api_key=\"dummy\"\n",
    "    )\n",
    "def get_response_from_model(Question):\n",
    "    response = client_agent.chat.completions.create(\n",
    "            model=\"meta/llama3-8b-instruct\",  # Replace with your model name if different\n",
    "            messages=[{'role': 'user', 'content': Question }],\n",
    "            temperature= 0,\n",
    "            top_p= 1,\n",
    "            max_tokens= 1024,\n",
    "            stream= True\n",
    "        )\n",
    "    model_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            model_response += chunk.choices[0].delta.content\n",
    "    return model_response.strip().lower()\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=get_response_from_model,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"NVIDIA NIM Model Interaction\",\n",
    "    description=\"Enter your question below:\"\n",
    ")\n",
    "\n",
    "# Start Gradio\n",
    "gr.TabbedInterface([interface], [\"NVIDIA NIM Model Interaction\"]).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
